apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: metadata-inference-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.6, pipelines.kubeflow.org/pipeline_compilation_time: '2021-08-07T21:46:54.070685',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "tf_model_gd_id"},
      {"name": "inputs_gd_id"}, {"name": "batch_size"}, {"name": "score"}], "name":
      "metadata_inference_pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.6}
spec:
  entrypoint: metadata-inference-pipeline
  templates:
  - name: download-and-extract-op
    container:
      args: []
      command: [python, ./src/download_zip_file.py, --gd_file_id, '{{inputs.parameters.tf_model_gd_id}}',
        --extracted_folder, /tmp/outputs/Extracted_Dataset/data]
      image: pedrohgv/download-and-extract-google-drive-op:latest
    inputs:
      parameters:
      - {name: tf_model_gd_id}
    outputs:
      artifacts:
      - {name: download-and-extract-op-Extracted-Dataset, path: /tmp/outputs/Extracted_Dataset/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.6
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Downloads
          and extracts file from google drive.", "implementation": {"container": {"command":
          ["python", "./src/download_zip_file.py", "--gd_file_id", {"inputValue":
          "Google Drive ID"}, "--extracted_folder", {"outputPath": "Extracted Dataset"}],
          "image": "pedrohgv/download-and-extract-google-drive-op:latest"}}, "inputs":
          [{"description": "File ID of file in Google Drive", "name": "Google Drive
          ID", "type": "String"}], "name": "Download and Extract op", "outputs": [{"description":
          "Folder containing images and annotaions in Pascal format", "name": "Extracted
          Dataset", "type": "Directory"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e38c8ba85f408bbb799291ec94fc1cd9efc85232f4c86c4a904ac542c45cd368", "url":
          "./download_and_extract_google_drive_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Google
          Drive ID": "{{inputs.parameters.tf_model_gd_id}}"}'}
  - name: download-and-extract-op-2
    container:
      args: []
      command: [python, ./src/download_zip_file.py, --gd_file_id, '{{inputs.parameters.inputs_gd_id}}',
        --extracted_folder, /tmp/outputs/Extracted_Dataset/data]
      image: pedrohgv/download-and-extract-google-drive-op:latest
    inputs:
      parameters:
      - {name: inputs_gd_id}
    outputs:
      artifacts:
      - {name: download-and-extract-op-2-Extracted-Dataset, path: /tmp/outputs/Extracted_Dataset/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.6
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Downloads
          and extracts file from google drive.", "implementation": {"container": {"command":
          ["python", "./src/download_zip_file.py", "--gd_file_id", {"inputValue":
          "Google Drive ID"}, "--extracted_folder", {"outputPath": "Extracted Dataset"}],
          "image": "pedrohgv/download-and-extract-google-drive-op:latest"}}, "inputs":
          [{"description": "File ID of file in Google Drive", "name": "Google Drive
          ID", "type": "String"}], "name": "Download and Extract op", "outputs": [{"description":
          "Folder containing images and annotaions in Pascal format", "name": "Extracted
          Dataset", "type": "Directory"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e38c8ba85f408bbb799291ec94fc1cd9efc85232f4c86c4a904ac542c45cd368", "url":
          "./download_and_extract_google_drive_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Google
          Drive ID": "{{inputs.parameters.inputs_gd_id}}"}'}
  - name: metadata-inference-pipeline
    inputs:
      parameters:
      - {name: batch_size}
      - {name: inputs_gd_id}
      - {name: score}
      - {name: tf_model_gd_id}
    dag:
      tasks:
      - name: download-and-extract-op
        template: download-and-extract-op
        arguments:
          parameters:
          - {name: tf_model_gd_id, value: '{{inputs.parameters.tf_model_gd_id}}'}
      - name: download-and-extract-op-2
        template: download-and-extract-op-2
        arguments:
          parameters:
          - {name: inputs_gd_id, value: '{{inputs.parameters.inputs_gd_id}}'}
      - name: predict-video-metadata-op
        template: predict-video-metadata-op
        dependencies: [download-and-extract-op, download-and-extract-op-2]
        arguments:
          parameters:
          - {name: batch_size, value: '{{inputs.parameters.batch_size}}'}
          - {name: score, value: '{{inputs.parameters.score}}'}
          artifacts:
          - {name: download-and-extract-op-2-Extracted-Dataset, from: '{{tasks.download-and-extract-op-2.outputs.artifacts.download-and-extract-op-2-Extracted-Dataset}}'}
          - {name: download-and-extract-op-Extracted-Dataset, from: '{{tasks.download-and-extract-op.outputs.artifacts.download-and-extract-op-Extracted-Dataset}}'}
  - name: predict-video-metadata-op
    container:
      args: []
      command: [python3, ./src/detect_video.py, --tf_model, /tmp/inputs/Tensorflow_Model/data,
        --inputs, /tmp/inputs/Input_Videos/data, --batch_size, '{{inputs.parameters.batch_size}}',
        --score, '{{inputs.parameters.score}}']
      image: pedrohgv/predict-metadata-op:latest
    inputs:
      parameters:
      - {name: batch_size}
      - {name: score}
      artifacts:
      - {name: download-and-extract-op-2-Extracted-Dataset, path: /tmp/inputs/Input_Videos/data}
      - {name: download-and-extract-op-Extracted-Dataset, path: /tmp/inputs/Tensorflow_Model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.6
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Performs
          an object detection task on videos and outputs metadata.", "implementation":
          {"container": {"command": ["python3", "./src/detect_video.py", "--tf_model",
          {"inputPath": "Tensorflow Model"}, "--inputs", {"inputPath": "Input Videos"},
          "--batch_size", {"inputValue": "Batch Size"}, "--score", {"inputValue":
          "Score"}], "image": "pedrohgv/predict-metadata-op:latest"}}, "inputs": [{"description":
          "Input path that contains Tensorflow model files and a config folder, which
          contains a obj.names file", "name": "Tensorflow Model", "type": "Directory"},
          {"description": "Folder containing videos to run the object detection task
          on.", "name": "Input Videos", "type": "Directory"}, {"default": "32", "description":
          "Batch size of frames to be loaded at each pass for through the model.",
          "name": "Batch Size", "type": "Integer"}, {"default": "0.7", "description":
          "Minimum score to be considered a valid detection.", "name": "Score", "type":
          "Float"}], "name": "Predict Video Metadata Op"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "637233c3bf8216d83c7c9a412ce81b859e60c33ea98b0914a1ff2372ed2b036e", "url":
          "./predict_metadata_op.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Batch
          Size": "{{inputs.parameters.batch_size}}", "Score": "{{inputs.parameters.score}}"}'}
  arguments:
    parameters:
    - {name: tf_model_gd_id}
    - {name: inputs_gd_id}
    - {name: batch_size}
    - {name: score}
  serviceAccountName: pipeline-runner
